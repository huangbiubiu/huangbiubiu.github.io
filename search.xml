<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>编写高质量的bash脚本</title>
    <url>/2020/TUTORIAL-TIPS-ON-BASH-SCRIPTS/</url>
    <content><![CDATA[<!-- # [TUTORIAL] Linux bash -->

<p>Linux bash 是提高效率的重要方法。本文介绍几个在编写bash脚本中有用的小技巧，能够有效提升脚本运行的健壮性和可维护性。</p>
<a id="more"></a>

<h2 id="日志和输出"><a href="#日志和输出" class="headerlink" title="日志和输出"></a>日志和输出</h2><p>日志是debug的重要手段。丰富的日志能够帮助我们快速定位问题所在，从而高效解决问题。</p>
<h3 id="打印命令"><a href="#打印命令" class="headerlink" title="打印命令"></a>打印命令</h3><p><code>set -x</code>指令能够让脚本向标准输出打印每一个执行的命令，这样一旦出错，可以通过打印的输出，找到错误执行的命令。</p>
<p>例子：</p>
<p>执行脚本：</p>
<figure class="highlight bash"><figcaption><span>print_trace.sh</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> -x</span><br><span class="line"><span class="built_in">echo</span> `expr 10 + 20 `</span><br></pre></td></tr></table></figure>

<p>输出为：</p>
<figure class="highlight plain"><figcaption><span>print_trace.out</span></figcaption><table><tr><td class="code"><pre><span class="line">+ expr 10 + 20</span><br><span class="line">+ echo 30</span><br><span class="line">30</span><br></pre></td></tr></table></figure>

<p>若想关闭这个功能，只需使用<code>set +x</code>即可：</p>
<p>执行脚本：</p>
<figure class="highlight bash"><figcaption><span>disable_trace.sh</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> -x</span><br><span class="line"><span class="built_in">echo</span> `expr 10 + 20 `</span><br><span class="line"><span class="built_in">set</span> +x</span><br><span class="line"><span class="built_in">echo</span> `expr 10 + 20 `</span><br></pre></td></tr></table></figure>

<p>输出为：</p>
<figure class="highlight plain"><figcaption><span>print_trace.out</span></figcaption><table><tr><td class="code"><pre><span class="line">+ expr 10 + 20</span><br><span class="line">+ echo 30</span><br><span class="line">30</span><br><span class="line">30</span><br></pre></td></tr></table></figure>

<blockquote>
<p>以上两个例子来自<a href="https://stackoverflow.com/a/36277661/5634636。" target="_blank" rel="noopener">https://stackoverflow.com/a/36277661/5634636。</a></p>
</blockquote>
<h4 id="重定向set-x的输出"><a href="#重定向set-x的输出" class="headerlink" title="重定向set -x的输出"></a>重定向<code>set -x</code>的输出</h4><p>我们希望将打印出来的命令重定向到日志文件中，以便留存和之后查看。此时可以使用<code>exec</code>命令，重定向脚本的输出：</p>
<figure class="highlight plain"><figcaption><span>redirect_output.sh bash</span></figcaption><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">exec &amp;&gt; &gt;(tee -a &quot;$&#123;terminal_output_dir&#125;&#x2F;terminal.out&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><code>tee -a</code>的作用是同时输出到文件和标准输出，这样我们可以在terminal看到输出，同时输出也会被保存到文件中。</p>
<blockquote>
<p>本方法来自<a href="https://unix.stackexchange.com/a/145654/283657。" target="_blank" rel="noopener">https://unix.stackexchange.com/a/145654/283657。</a></p>
</blockquote>
<h3 id="有颜色的输出"><a href="#有颜色的输出" class="headerlink" title="有颜色的输出"></a>有颜色的输出</h3><p>在执行危险操作时，我们希望有更醒目的警示来提示用户。例如在删除文件时，我们希望用红色的字体来提示用户。在执行成功时，我们通常使用绿色的字体。如何打印带颜色的字体？</p>
<blockquote>
<p>本方法来自<a href="https://stackoverflow.com/a/20983251/5634636。" target="_blank" rel="noopener">https://stackoverflow.com/a/20983251/5634636。</a></p>
</blockquote>
<p>首先预先定义三个变量：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">RED=$(tput setaf 1)</span><br><span class="line">GREEN=$(tput setaf 2)</span><br><span class="line">RESET=$(tput sgr0)</span><br></pre></td></tr></table></figure>

<p>然后就可以自由地在字符串中使用了：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo &quot;$&#123;RED&#125;ERROR! SOMETHING WRONG!$&#123;RESET&#125;&quot;</span><br></pre></td></tr></table></figure>

<p><strong>实践</strong>：在删除危险文件前警示用户：</p>
<figure class="highlight bash"><figcaption><span>remove_dangerous_files.sh</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set color</span></span><br><span class="line">RED=$(tput setaf 1)</span><br><span class="line">GREEN=$(tput setaf 2)</span><br><span class="line">RESET=$(tput sgr0)</span><br><span class="line"></span><br><span class="line"><span class="built_in">read</span> -p <span class="string">"Are you sure to remove files?<span class="variable">$&#123;RESET&#125;</span>"</span> -n 1 -r</span><br><span class="line"><span class="built_in">echo</span>    <span class="comment"># move to a new line</span></span><br><span class="line"><span class="keyword">if</span> [[ <span class="variable">$REPLY</span> =~ ^[Yy]$ ]]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">    <span class="comment"># do dangerous stuff</span></span><br><span class="line">    rm -rf ./output_files</span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>

<h2 id="更健壮的脚本"><a href="#更健壮的脚本" class="headerlink" title="更健壮的脚本"></a>更健壮的脚本</h2><p>bash有的行为和可能会带来意想不到的结果，甚至带来灾难性的后果。我们可以用一些方法来预先阻止这样令人意外的行为。</p>
<h3 id="出错即停止"><a href="#出错即停止" class="headerlink" title="出错即停止"></a>出错即停止</h3><p>和大多数程序语言报错即退出不同，在执行bash脚本时，如果某行命令报错，bash会接着执行之后的指令。如果没有考虑到这一点，可能会带来非预期的效果。例如我们有一个简单的删除日志文件的脚本：</p>
<figure class="highlight bash"><figcaption><span>clear_log.sh</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ./logs</span><br><span class="line">rm -rf *</span><br></pre></td></tr></table></figure>

<p>这段代码中，如果<code>./logs</code>目录因为某种原因不存在，<code>cd ./logs</code>就会报错，然后<strong>继续执行</strong><code>rm -rf *</code>命令，这是十分危险的，有可能将整个路径都删除。</p>
<p>为了预防这样的错误，我们令脚本在任意一行命令出错后即停止，不再接着执行接下来的命令。只需要加一行<code>set -e</code>即可：</p>
<figure class="highlight bash"><figcaption><span>save_clear_log.sh</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> -e</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> ./logs</span><br><span class="line">rm -rf *</span><br></pre></td></tr></table></figure>

<p>在<code>save_clear_log.sh</code>中，如果<code>cd ./logs</code>报错，该程序会退出，删除的命令将不会被执行。</p>
<h3 id="禁止使用未定义变量"><a href="#禁止使用未定义变量" class="headerlink" title="禁止使用未定义变量"></a>禁止使用未定义变量</h3><p>在Linux中，使用未定义的变量是被允许的，如果一个变量未被定义，则自动替换为空字符串。这在有时候是危险的：</p>
<figure class="highlight bash"><figcaption><span>clear_log.sh</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> -e</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$&#123;log_path&#125;</span></span><br><span class="line">rm -rf *</span><br></pre></td></tr></table></figure>

<p>此时如果<code>${log_path}</code>变量未定义，就会执行<code>cd</code>，当前目录就会跳转至用户home目录（即等价于<code>cd ~</code>）。这样这个脚本会将用户home目录中所有文件全部删除。</p>
<p>为了预防这样的错误，我们可以禁止将未定义变量展开为空字符串：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> -u</span><br></pre></td></tr></table></figure>

<p>脚本可以写为</p>
<figure class="highlight bash"><figcaption><span>clear_log.sh</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> -eu</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$&#123;log_path&#125;</span></span><br><span class="line">rm -rf *</span><br></pre></td></tr></table></figure>

<p>此时若<code>${log_path}</code>变量未定义，<code>cd ${log_path}</code>就会报错，同时因为<code>set -e</code>的缘故，整个脚本将会退出，不再执行删除命令。</p>
<blockquote><p><code>set -eu</code> 等价于</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> -e</span><br><span class="line"><span class="built_in">set</span> -u</span><br></pre></td></tr></table></figure></blockquote>

<h3 id="确保本地git仓库和远端保持一致"><a href="#确保本地git仓库和远端保持一致" class="headerlink" title="确保本地git仓库和远端保持一致"></a>确保本地git仓库和远端保持一致</h3><p>在运行代码时，特别是在本地编写代码在服务器运行的情况，如果在本地修改好，却忘记在服务器端pull下来，就会运行非预期的代码。我们可以在脚本运行时首先检查本地代码是否与远端一致：</p>
<figure class="highlight bash"><figcaption><span>check_git_status.sh</span></figcaption><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># check git status</span></span><br><span class="line"><span class="comment"># see https://stackoverflow.com/questions/3258243/check-if-pull-needed-in-git</span></span><br><span class="line"><span class="comment"># check if the dir is git repo</span></span><br><span class="line"><span class="comment"># see https://stackoverflow.com/a/38088814/5634636</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$(git rev-parse --is-inside-work-tree 2&gt;/dev/null)</span>"</span> == <span class="string">"true"</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">'Checking git status...'</span></span><br><span class="line">  git fetch</span><br><span class="line">  LOCAL=$(git rev-parse @)</span><br><span class="line">  REMOTE=$(git rev-parse <span class="string">"@&#123;u&#125;"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> [ <span class="variable">$LOCAL</span> = <span class="variable">$REMOTE</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"<span class="variable">$&#123;GREEN&#125;</span>Git is up-to-date<span class="variable">$&#123;RESET&#125;</span>"</span></span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"<span class="variable">$&#123;GREEN&#125;</span>Git is NOT up-to-date! Pull from remote first!<span class="variable">$&#123;RESET&#125;</span>"</span></span><br><span class="line">    <span class="built_in">exit</span> 1</span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"Not in git repo."</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Further-Reading"><a href="#Further-Reading" class="headerlink" title="Further Reading"></a>Further Reading</h2><ul>
<li><a href="https://mp.weixin.qq.com/s?__biz=MjM5ODYwMjI2MA==&mid=2649745834&idx=1&sn=a87c1abec3395327436cc812c8579188&chksm=bed37ad189a4f3c727a9133beaed00d98b78c9dbf7def2f1c55615183eb26eda0fc1be61e93d&mpshare=1&scene=1&srcid=&sharer_sharetime=1586259448506&sharer_shareid=15f38622a00e2c24f3fbd2ab83530e29&key=c6bd44209e6ec513afcc83a4d322a02f2414e6631892457d1fd9a9f5df865f618b840897ee3bf5394da73b3539b1ff233aa1ba186777c8e843fb8ae4bf8a8ca1983f711e652d0a3b62ad898b2462d513&ascene=1&uin=Mjg4MTEzNDIzNA%3D%3D&devicetype=Windows+10&version=62080079&lang=en&exportkey=A9u20MEhMFoSLKvdqc1d2N0%3D&pass_ticket=8B73Soawz1nfoUCSywUtATistyycGq8JWg6gYSlq0nYhQ%2B9TVkURAFntyNNUtubm" target="_blank" rel="noopener">编写可靠 bash 脚本的一些技巧</a></li>
<li><a href="https://www.gnu.org/software/bash/manual/html_node/The-Set-Builtin.html" target="_blank" rel="noopener">The Set Builtin (Bash Reference Manual)</a></li>
</ul>
]]></content>
      <categories>
        <category>TUTORIAL</category>
      </categories>
      <tags>
        <tag>TUTORIAL</tag>
        <tag>bash</tag>
      </tags>
  </entry>
  <entry>
    <title>Regularization and Shrinkage</title>
    <url>/2020/TUTORIAL-REGULARIZATION-AND-SHRINKAGE/</url>
    <content><![CDATA[<p>为什么把参数变小有助于防止过拟合？</p>
<a id="more"></a>

<p>正则化(Regularization)是机器学习中常见的技术。一种常见的正则化技术是$\ell_2$-Regularization (又称作weight decay). 这类正则化基本的思路就是在最终损失函数中加一个$\ell_2$正则化项，使得参数尽量变小：</p>
<p>$$L(\boldsymbol{f}(\boldsymbol{x};\boldsymbol{\theta}),\boldsymbol{y})=L(\boldsymbol{x},\boldsymbol{y})+\lambda \lVert \boldsymbol{\theta} \rVert_2$$</p>
<p>我们将$\lambda$称为<em>正则化系数</em>。</p>
<p>一种常见的说法是，正则化通过减小模型的复杂性来降低过拟合的可能。参数少的模型具有较低的复杂性，这听起来是容易理解的。然而，我们通常使用基于梯度的方法(如SGD)来优化损失函数。对于$\theta_i$, $\ell_2$正则化项的导数是$2\theta_i$。这使得在计算$\theta_i$梯度的时候随着$\theta_i$的减小，$\frac{dL}{d\theta_i}$会越来越小，导致$\theta_i$只能无限<strong>趋近于</strong>$0$，而很难<strong>等于</strong>$0$. 这时，利用$\ell_2$正则化降低模型复杂性就显得难以理解了。参数<strong>小</strong>和<strong>没有</strong>似乎是完全不同的两件事。一个具有非常多参数的模型，即使这些参数的值很小，但他们参数空间和参数值很大的情况下是一样的。正则化是如何降低模型的复杂度的？</p>
<h2 id="解释1-Bias-variance-tradeoff"><a href="#解释1-Bias-variance-tradeoff" class="headerlink" title="解释1: Bias-variance tradeoff"></a>解释1: Bias-variance tradeoff</h2><p>这个理论的基本思想是，$\ell_p$正则化通过放弃估计的<em>无偏性</em>来获取更小的<em>方差</em>。我们在附录A中简单介绍了无偏性和方差的基本概念。简单来说，无偏性(unbias)代表了估计均值与真实值的差异，方差(variance)代表估计的离散程度。</p>
<p>为什么我们需要更小的方差？直观来说，一个无偏的估计在训练集上能够达到最好的效果（估计的期望与<strong>训练集</strong>的期望保持一致），但这样的估计显然可能不适用于测试集。尤其是在数据集具有较大方差的情况下，训练集和测试集有着较大的差异，就导致估计器（或模型）在训练集上有着优良的表现，而在测试集上表现很差，这就是所谓的<em>过拟合</em>。</p>
<p>为什么需要放弃无偏性来获取更小的方差？高斯-马尔科夫定理(附录B)指出，如果误差满足零均值、同方差且互不相关，最小二乘法估计就是最佳线性无偏估计，即最小二乘回归(Ordinary Least Squares, OLS)在所有无偏估计中有着最小的方差。但是，<strong>方差<em>较低<em>不意味着方差</em>非常低</em></strong>。在<a href="https://stats.stackexchange.com/a/20303/182826" target="_blank" rel="noopener">一些情况</a>下，OLS估计的方差会非常大，导致OLS的结果毫无作用。如果这个估计仍然存在过拟合的情况，我们只能考虑放弃无偏性来获得更优的估计。我们希望通过放弃无偏性，获得一种有偏估计，这种估计的期望可能略微和真实值有所偏离，但他获得了更小的方差。</p>
<h3 id="Bias–variance-decomposition"><a href="#Bias–variance-decomposition" class="headerlink" title="Bias–variance decomposition"></a>Bias–variance decomposition</h3><blockquote><p><strong>Notation</strong></p>
<p>$f$: $\boldsymbol{f}(\boldsymbol{x};\boldsymbol{\theta})$, 估计的真实值<br>$\hat{f}$: $\boldsymbol{f}(\boldsymbol{x};\hat{\boldsymbol{\theta}})$, 模型估计的值<br>$y$: 模型的标签，是真实值和噪声$\varepsilon$的叠加: $y=f+\varepsilon$</p>
</blockquote>

<p>考虑Squared Loss:<br>$$L_{Square}(\hat{f},\boldsymbol{y})=\Bbb{E}[(\hat{f}-\boldsymbol{y})^2]$$</p>
<p>Squared Loss<a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#Derivation" target="_blank" rel="noopener">可以被分解为Bias, Variance和Noise之和</a>:</p>
<p>$$L_{Square}(\hat{f},\boldsymbol{y})=Bias[\hat{f}]^2+Var[y]+Var[\hat{f}]$$</p>
<p>可以看出，Square Loss是由偏差, 方差和数据噪声三个部分共同组成的。数据噪声是数据集带来的（例如人工标注错误等），不在模型的控制范围内。我们调节Bias和Variance，就是在这两者直接取一个均衡，来获得最小的损失。</p>
<h3 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h3><p>这个理论的限制在于，Bias–variance decomposition基于的是多个独立的数据集多次实验的平均结果，而实际应用中，我们往往只有单一的数据集和单次的实验结果。如果我们有更大的数据集，我们完全不需要分割开做多次实验，而是一次用一个非常大的数据集来训练模型，这样往往能够有效降低过拟合。</p>
<h3 id="Further-Reading"><a href="#Further-Reading" class="headerlink" title="Further Reading"></a>Further Reading</h3><ol>
<li>Christopher M. Bishop, <em>Pattern Recognition and Machine Learning</em>, Section 3.2: The Bias-Variance Decomposition</li>
</ol>
<h2 id="解释2-Shrinkage-Estimation"><a href="#解释2-Shrinkage-Estimation" class="headerlink" title="解释2: Shrinkage Estimation"></a>解释2: Shrinkage Estimation</h2><p>我们从Stein Estimator谈起。直觉上说，James–Stein估计器表达了这样一个思想：将参数<em>收缩</em>可以获得更好的估计。</p>
<p><strong>例子：估计一组随机变量的均值$\boldsymbol{\theta}$</strong></p>
<blockquote><p><strong>问题</strong></p>
<p>考虑$p$个随机变量$X_1,…,X_p$服从正态分布$X_i \sim N(\theta_i,1)$，每个随机变量方差均为$1$，但均值$\theta_i$各不相同。</p>
<p>找到一个对随机变量分布期望的估计$\hat{\boldsymbol{\theta}}=\hat{\boldsymbol{\theta}}(X)$, 其中$\boldsymbol{\theta}=(\theta_1,…,\theta_p)^T$。</p>
</blockquote>

<p><strong>损失函数</strong> 我们使用Squared Loss作为损失函数:<br>$$L(\hat{\theta},\theta)=\lVert \hat{\boldsymbol{\theta}} - \boldsymbol{\theta} \rVert_2$$</p>
<p>由于$\boldsymbol{\theta}$是随机变量，具体到某个数据集，我们考虑<em>风险函数</em>(risk function):<br>$$R(\hat{\theta},\theta)=\Bbb{E}[L(\hat{\theta},\theta)]$$</p>
<p><strong>一个简单的估计器</strong> 一种非常容易想到的估计器是$\hat{\boldsymbol{\theta}}(X)=X$. 由于$\Bbb{E}[X_i]=\theta_i$, 因此$X_i$是$\theta_i$的一个<em>无偏估计</em>(附录A)。 实际上，这个估计就是对$\theta$的MLE估计（附录C）。</p>
<p><strong>Stein Estimator</strong> 在$p&gt;2$时，Stein证明了，下述Stein估计的效果好于(dominate)好于MLE估计:</p>
<p>$$\hat{\theta}^{J S}(X)=\left(1-\frac{p-2}{|X|^{2}}\right) X$$</p>
<blockquote>
<p>注: 实际上也好于MLP估计，参见Efron et al., <em>Computer Age Statistical Inference: Algorithms, Evidence and Data Science</em>, Section 7.1: The James–Stein Estimator.</p>
</blockquote>
<p>$\left(1-\frac{p-2}{|X|^{2}}\right)$对$X$起到收缩的作用。在当时(1960s)的统计学界，这一发现无疑是令人震惊的。这一发现直击MLE的关键阵地：正态分布和Squared Loss。在维度低的情况下，或许有道理考虑MLE估计，但在现代数据中的<strong>高维</strong>情况下，不考虑收缩分布是没有道理的。</p>
<p>在此之后，大量收缩估计(Shrinkage Estimator)被开发出来，其中最具有代表性的就是岭回归(Ridge Regression, $\ell_2$正则化)和Lasso回归($\ell_1$正则化)。</p>
<blockquote><p><strong>为什么几乎没有人直接使用Stein Estimator?</strong></p>
<p>一个原因是，目前没有很好针对Stein Estimator的优化方法。</p>
</blockquote>

<h3 id="Further-Reading-1"><a href="#Further-Reading-1" class="headerlink" title="Further Reading"></a>Further Reading</h3><ol>
<li>Bradley Efron and Trevor Hastie, <em>Computer Age Statistical Inference: Algorithms, Evidence and Data Science</em>, Section 7: James–Stein Estimation and Ridge Regression</li>
<li>Richard J. Samworth, <em>Stein’s Paradox</em></li>
<li><a href="https://stats.stackexchange.com/questions/122062/unified-view-on-shrinkage-what-is-the-relation-if-any-between-steins-paradox" target="_blank" rel="noopener">Unified view on shrinkage: what is the relation (if any) between Stein’s paradox, ridge regression, and random effects in mixed models?</a></li>
</ol>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>现如今，正则化几乎成为了机器学习方法的标准配置，从岭回归到神经网络，正则化都发挥着不可或缺的作用。本文从两个不同的方向介绍了regularization的基本思想，希望给读者带来一些启示。</p>
<h2 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h2><h3 id="Appendix-A-无偏性和方差"><a href="#Appendix-A-无偏性和方差" class="headerlink" title="Appendix A: 无偏性和方差"></a>Appendix A: 无偏性和方差</h3><p>统计学中的“偏差” (Bias) 指的是估计量$\hat{\theta}$的<strong>期望</strong>与真实值$\theta$之差:<br>$$\Bbb{E}[\hat{\theta}]-\theta$$</p>
<p>Bias为零的估计被称为“无偏估计”，Bias不为零的估计被称为“有偏估计”。Variance是容易理解的，即多次$\hat{\theta}$估计的离散程度。Bias是对<strong>期望</strong>的衡量，Variance是对<strong>方差</strong>的衡量。我们希望$\hat{\theta}$满足:</p>
<ul>
<li>Bias为0. 即所有估计量的期望$\Bbb{E}[\hat{\theta}]$恰好等于真实值$\theta$.</li>
<li>Variance为0. 即每次估计都得到稳定一致的估计量$\hat{\theta}$.</li>
</ul>
<h3 id="Appendix-B-高斯-马尔科夫定理-Gauss-Markov-Theorem"><a href="#Appendix-B-高斯-马尔科夫定理-Gauss-Markov-Theorem" class="headerlink" title="Appendix B: 高斯-马尔科夫定理 (Gauss-Markov Theorem)"></a>Appendix B: 高斯-马尔科夫定理 (Gauss-Markov Theorem)</h3><p>高斯-马尔科夫定理：</p>
<p>在线性回归模型中，如果误差满足零均值、同方差且互不相关，则回归系数的最佳线性无偏估计(BLUE, Best Linear unbiased estimator)就是<a href="https://zh.wikipedia.org/wiki/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95" target="_blank" rel="noopener">最小二乘法估计</a>。</p>
<h3 id="Appendix-C-MLE-and-MAP"><a href="#Appendix-C-MLE-and-MAP" class="headerlink" title="Appendix C: MLE and MAP"></a>Appendix C: MLE and MAP</h3><h4 id="最大似然估计-Maximum-Likelihood-Estimation-MLE"><a href="#最大似然估计-Maximum-Likelihood-Estimation-MLE" class="headerlink" title="最大似然估计 (Maximum Likelihood Estimation, MLE)"></a>最大似然估计 (Maximum Likelihood Estimation, MLE)</h4><p>对于一组数据$D$, 最大似然估计分为以下两步：</p>
<ol>
<li>对数据的分布作出假设: 数据是从何种分布中采样而来的。即$P(D;\theta)$, 其中$\theta$是分布的参数。</li>
</ol>
<p>例子：对于抛硬币的结果（正或者反），假设数据从<a href="https://en.wikipedia.org/wiki/Binomial_distribution" target="_blank" rel="noopener">二项分布</a>中采样，参数$\theta$就是每次实验中抛出正面的概率。</p>
<ol start="2">
<li><strong>怎样的$\theta$能够使得这个数据最有可能存在</strong>？即最大化$P(D;\theta)$:<br>$$\hat{\theta}_{MLE}=\underset{\theta}{\operatorname{argmax}} P(D ; \theta)$$</li>
</ol>
<h4 id="最大后验概率-Maximum-a-Posteriori-Probability-Estimation-MAP"><a href="#最大后验概率-Maximum-a-Posteriori-Probability-Estimation-MAP" class="headerlink" title="最大后验概率 (Maximum a Posteriori Probability Estimation, MAP)"></a>最大后验概率 (Maximum a Posteriori Probability Estimation, MAP)</h4><p>贝叶斯方法与MLE最大的不同在于，MLE将数据$D$的概率分布记为$P(D;\theta)$（此时$\theta$是一个<strong>参数</strong>），而贝叶斯方法将数据$D$的概率分布记为<strong>$P(D|\theta)$</strong>（此时$\theta$是一个<strong>随机变量</strong>）。</p>
<blockquote>
<p>$\theta$并不与一个随机事件相关联，也不对应一个采样空间，在频率学派统计中，这不能成为一个随机变量，而在贝叶斯统计中是允许的。</p>
</blockquote>
<p>贝叶斯统计中，我们把$\theta$服从的分布$P(\theta)$叫做<em>先验</em> (prior)，即在我们没有观察数据$D$的时候对参数$\theta$服从分布的预备知识；$P(D|\theta)$叫做<em>似然</em>(likelihood)，即给定$\theta$的情况下这个数据存在的可能性；$P(\theta|D)$叫做<em>后验</em> (posterior)，即我们在已经观察到数据之后对$\theta$的概率估计。</p>
<p>MLE就是最大化似然函数$P(D|\theta)$，MAP就是最大化后验概率$P(\theta|D)$：</p>
<p>$$\hat{\theta}_{MAP}=\underset{\theta}{\operatorname{argmax}} P(\theta | D)$$</p>
<blockquote><p><strong>先验分布从哪里来？</strong></p>
<p>贝叶斯估计要求预先知道参数的先验分布$P(\theta)$，否则无法进行估计。那么我们怎么知道先验分布呢？通常来说，我们根据自己的知识<em>假设</em>一个先验分布（即使他不一定对）。</p>
<ol>
<li>抛硬币：$\theta$表示每次抛硬币正面向上的概率。根据经验，$\theta$的分布应当满足：(a) $P(\theta)$最大值应在$P(\theta=0.5)$处；(b) $\theta \in [0, 1]$；(c) $\theta$的分布类似于一个钟形分布，在$0.5$处最大，$0$和$1$处最小。根据上述三个条件，我们选择<a href="https://en.wikipedia.org/wiki/Beta_distribution" target="_blank" rel="noopener">Beta分布</a>.</li>
<li>均值的先验分布：我们希望估计高斯分布$N(\theta,1)$的均值$\theta$，由于高斯分布是常见的分布，我们可以假设$\theta$服从高斯分布$\theta \sim N(M,N)$。</li>
</ol>
<p>此外，也可以从数据中估计先验分布，请参考<a href="https://en.wikipedia.org/wiki/Empirical_Bayes_method" target="_blank" rel="noopener">经验贝叶斯</a>.</p>
</blockquote>

<h4 id="Further-Reading-2"><a href="#Further-Reading-2" class="headerlink" title="Further Reading"></a>Further Reading</h4><ol>
<li>Kilian Weinberger, Lecture 7 <em>Estimating Probabilities from Data: Maximum Likelihood Estimation</em> - Cornell CS4780 SP17, <a href="https://www.youtube.com/watch?v=RIawrYLVdIw&amp;t=17s" target="_blank" rel="noopener">https://www.youtube.com/watch?v=RIawrYLVdIw&amp;t=17s</a></li>
<li>Bradley Efron and Trevor Hastie, <em>Computer Age Statistical Inference: Algorithms, Evidence and Data Science</em>, Section 6: Empirical Bayes</li>
</ol>
]]></content>
      <categories>
        <category>TUTORIAL</category>
      </categories>
      <tags>
        <tag>TUTORIAL</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Faster Data Loading in PyTorch</title>
    <url>/2019/TUTORIAL-FASTER-DATA-LOADING/</url>
    <content><![CDATA[<p><em>不要让数据加载限制你的训练速度！</em></p>
<p>教程：如何避免训练速度被数据加载拖累</p>
<a id="more"></a>

<p>数据加载是神经网络训练的重要步骤。我们会花费大量金钱来购买更加昂贵的GPU，以求获得更快的训练（推理）速度。然而，数据加载的时间花费却获得较少的关注。如何优化数据加载的过程，从而充分利用GPU?</p>
<p><strong>TL;DR</strong>: <a href="#Checklist">Checklist</a></p>
<h2 id="是不是：数据加载的时间是你的瓶颈吗？"><a href="#是不是：数据加载的时间是你的瓶颈吗？" class="headerlink" title="是不是：数据加载的时间是你的瓶颈吗？"></a>是不是：数据加载的时间是你的瓶颈吗？</h2><p>「先问是不是」：解决这个问题之前，我们首先需要知道数据加载的时间是否真的拖慢了训练速度。下面介绍几个方法来查看训练中数据加载花费的时间。</p>
<h3 id="直接测量-time-time"><a href="#直接测量-time-time" class="headerlink" title="直接测量: time.time()"></a>直接测量: <code>time.time()</code></h3><p>最为直观的方法就是测量数据加载所需要的时间花费。Python中可以使用<code>time.time()</code>来返回当前的时间。在数据加载前后分别测量一次当前时间，相减后即为数据加载时间。PyTorch提供了一个很好的<a href="https://github.com/pytorch/examples/blob/master/imagenet/main.py#L277-L280" target="_blank" rel="noopener">示例</a>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># only code snippet</span></span><br><span class="line"><span class="comment"># could not run directly</span></span><br><span class="line"></span><br><span class="line">end = time.time()</span><br><span class="line"><span class="keyword">for</span> i, (images, target) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">    <span class="comment"># measure data loading time</span></span><br><span class="line">    data_time.update(time.time() - end)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>在Python中，更加精确地测量代码运行时间的方法是<code>timeit.timeit()</code>。<code>timeit</code>允许你选择不同的时间函数（例如<a href="https://docs.python.org/3.7/library/time.html#time.process_time" target="_blank" rel="noopener"><code>time.process_time()</code></a>来测量<em>CPU时间</em>而不是<em>当前时间</em>），以及重复多次运行来消除测量误差等，详见StackOverflow的<a href="https://stackoverflow.com/questions/17579357/time-time-vs-timeit-timeit" target="_blank" rel="noopener">相关讨论</a>. 而对于本任务，我相信<code>time.time()</code>的精确度是足够的，而且只需要对代码做微小的改动即可。有关使用<code>timeit.timeit()</code>测量代码速度的例子，可以参考<a href="https://docs.python.org/3.7/library/timeit.html#basic-examples" target="_blank" rel="noopener">文档</a>以及<a href="https://huangbiubiu.github.io/2019/BEST-PRACTICE-PyTorch-TensorDataset/#%E4%BB%A3%E7%A0%811-%E8%AF%84%E4%BC%B0%E4%BB%A3%E7%A0%81%E8%BF%90%E8%A1%8C%E9%80%9F%E5%BA%A6" target="_blank" rel="noopener">这篇博客</a>。</p>
</blockquote>
<blockquote>
<p><strong>注意</strong> 此方法应当运行多次后，以测量结果稳定后的数值为准。</p>
</blockquote>
<h3 id="使用Profile工具-cProfile"><a href="#使用Profile工具-cProfile" class="headerlink" title="使用Profile工具: cProfile"></a>使用Profile工具: cProfile</h3><p><a href="https://docs.python.org/3.7/library/profile.html" target="_blank" rel="noopener">cProfile</a>是Python提供的一个分析器，可以测量出不同函数被调用的次数，时间等信息。</p>
<p>这是一个数据加载时间缓慢的代码片段（来自于<a href="https://huangbiubiu.github.io/2019/BEST-PRACTICE-PyTorch-TensorDataset/" target="_blank" rel="noopener">这篇文章</a>）:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> timeit</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset, DataLoader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_data</span><span class="params">()</span> -&gt; DataLoader:</span></span><br><span class="line">    batch_size = <span class="number">8192</span></span><br><span class="line"></span><br><span class="line">    data_all = np.random.rand(batch_size * <span class="number">100</span>, <span class="number">128</span>)  <span class="comment"># demo input</span></span><br><span class="line">    dataset = TensorDataset(torch.from_numpy(data_all))</span><br><span class="line">    data_loader = DataLoader(dataset=dataset,</span><br><span class="line">                             shuffle=<span class="literal">True</span>,</span><br><span class="line">                             batch_size=<span class="number">8192</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> data_loader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iterate_data</span><span class="params">(dataloader)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i, x <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">        <span class="comment"># training</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dataloader = prepare_data()</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    print(timeit.timeit(<span class="string">'iterate_data(dataloader)'</span>, globals=globals(), number=<span class="number">5</span>))</span><br></pre></td></tr></table></figure>

<p>将这段代码保存为<code>test.py</code>文件，执行命令:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python -m cProfile -s time test.py</span><br></pre></td></tr></table></figure>

<p>程序运行结束后得到如下结果（截取部分）:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">44.836229782085866</span><br><span class="line">         20666773 function calls (20660872 primitive calls) in 50.746 seconds</span><br><span class="line"></span><br><span class="line">   Ordered by: internal time</span><br><span class="line"></span><br><span class="line">   ncalls  tottime  percall  cumtime  percall filename:lineno(function)</span><br><span class="line">  8192000   20.637    0.000   20.637    0.000 dataset.py:162(&lt;genexpr&gt;)</span><br><span class="line">  4096000    8.601    0.000   29.238    0.000 dataset.py:161(__getitem__)</span><br><span class="line">      500    8.044    0.016    8.044    0.016 &#123;built-in method stack&#125;</span><br><span class="line">      500    3.585    0.007   32.823    0.066 fetch.py:44(&lt;listcomp&gt;)</span><br><span class="line">    43&#x2F;41    2.715    0.063    2.721    0.066 &#123;built-in method _imp.create_dynamic&#125;</span><br><span class="line">      505    1.417    0.003    2.467    0.005 sampler.py:198(__iter__)</span><br><span class="line">        1    1.303    1.303    1.303    1.303 &#123;method &#39;rand&#39; of &#39;numpy.random.mtrand.RandomState&#39; objects&#125;</span><br><span class="line">      505    0.847    0.002   44.629    0.088 dataloader.py:344(__next__)</span><br><span class="line">      404    0.844    0.002    0.844    0.002 &#123;method &#39;read&#39; of &#39;_io.FileIO&#39; objects&#125;</span><br><span class="line">4101414&#x2F;4101115    0.354    0.000    0.354    0.000 &#123;built-in method builtins.len&#125;</span><br><span class="line">        1    0.328    0.328    0.328    0.328 &#123;built-in method mkl._py_mkl_service.get_version&#125;</span><br><span class="line">  4101636    0.317    0.000    0.317    0.000 &#123;method &#39;append&#39; of &#39;list&#39; objects&#125;</span><br><span class="line"> 1000&#x2F;500    0.234    0.000    8.347    0.017 collate.py:42(default_collate)</span><br><span class="line">        5    0.202    0.040    0.202    0.040 &#123;method &#39;tolist&#39; of &#39;torch._C._TensorBase&#39; objects&#125;</span><br><span class="line">      404    0.184    0.000    1.028    0.003 &lt;frozen importlib._bootstrap_external&gt;:914(get_data)</span><br><span class="line">        5    0.178    0.036    0.178    0.036 &#123;built-in method randperm&#125;</span><br><span class="line">      500    0.143    0.000   41.313    0.083 fetch.py:42(fetch)</span><br><span class="line">        5    0.139    0.028   44.830    8.966 test.py:20(iterate_data)</span><br><span class="line">        1    0.092    0.092    0.092    0.092 &#123;built-in method from_numpy&#125;</span><br><span class="line">      500    0.058    0.000    8.108    0.016 collate.py:79(&lt;listcomp&gt;)</span><br><span class="line">      404    0.051    0.000    0.051    0.000 &#123;built-in method marshal.loads&#125;</span><br><span class="line">        5    0.038    0.008    0.038    0.008 &#123;method &#39;random_&#39; of &#39;torch._C._TensorBase&#39; objects&#125;</span><br><span class="line">        9    0.035    0.004    0.035    0.004 &#123;method &#39;readline&#39; of &#39;_io.BufferedReader&#39; objects&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到每个函数被调用的时间和次数。从这个结果我们可以看到，在100次迭代中，<a href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class" target="_blank" rel="noopener"><code>__getitem__</code></a>函数被调用了4096000次，花费时间29.238s。</p>
<blockquote><p><strong>Note</strong>: PyCharm 用户 </p>
<p>PyCharm Professional为用户提供了图形界面的cProfile工具，详见<a href="https://www.jetbrains.com/help/pycharm/profiler.html" target="_blank" rel="noopener">文档</a>. </p>
</blockquote>


<blockquote><p><strong>Note</strong>: 可视化Profile结果</p>
<p>对于命令行用户，有许多软件可以提供可视化的cProfile结果，更加直观地分析代码运行情况，例如<a href="https://jiffyclub.github.io/snakeviz/" target="_blank" rel="noopener">SnakeViz</a>.</p>
</blockquote>

<blockquote><p><strong>Note</strong>: TensorFlow用户</p>
<p>在PyTorch中，PyTorch基于Python管理变量，因此Python的profile对PyTorch也同样有效。然而，如果对TensorFlow代码使用Python的profile工具，大概率只能看见<code>session.run()</code>占用了最大的运行时间，而无法看到网络内部op的执行时间。若希望对TensorFlow模型内部运算进行分析，请参考<a href="https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras" target="_blank" rel="noopener">TensorFlow Profiler</a>. 不过，TensorFlow的数据预处理通常也是基于Python完成，若仅仅对数据预处理部分分析，则可以直接使用Python的profile工具。</p>
</blockquote>

<h3 id="间接观察：nvidia-smi"><a href="#间接观察：nvidia-smi" class="headerlink" title="间接观察：nvidia-smi"></a>间接观察：nvidia-smi</h3><p>我们也可以通过不断观察<code>nvidia-smi</code>的GPU利用率字段来推断GPU是否处于满负荷状态：</p>
<p><code>watch -n0 nvidia-smi</code></p>
<p><a href="https://en.wikipedia.org/wiki/Watch_(Unix)" target="_blank" rel="noopener">该命令</a>每0.1s刷新一次<code>nvidia-smi</code>的状态。如果发现<code>Volatile GPU-Util</code>字段不是一直处于100%，有几种可能：</p>
<ol>
<li>程序对于GPU太弱了，GPU不需要满负荷运转就可以轻松应付你的程序；</li>
<li>这个字段不能准确反映GPU利用率，请参考<a href="https://stackoverflow.com/a/40938696/5634636" target="_blank" rel="noopener">这里的讨论</a>；</li>
<li>数据加载等操作阻塞了GPU的运算。</li>
</ol>
<p>第三种情况才是我们需要关注的情况。这种情况下，GPU利用率会呈现“过山车”式的曲线，一会达到100%（GPU运算，模型推理过程），一会非常低（被数据加载代码阻塞）。</p>
<h2 id="为什么：什么操作会拖慢数据加载？"><a href="#为什么：什么操作会拖慢数据加载？" class="headerlink" title="为什么：什么操作会拖慢数据加载？"></a>为什么：什么操作会拖慢数据加载？</h2><h3 id="数据增强和预处理"><a href="#数据增强和预处理" class="headerlink" title="数据增强和预处理"></a>数据增强和预处理</h3><p>普通的数据增强(例如<a href="https://pytorch.org/docs/stable/torchvision/transforms.html" target="_blank" rel="noopener"><code>torchvision.transforms</code></a>中的transformer)通常比较快。如果你使用了额外的数据增强和预处理方法，请着重关注他们的执行效率。这样的速度减缓通常可以在上文提到的<code>cProfile</code>的结果中反映出来。</p>
<h3 id="IO"><a href="#IO" class="headerlink" title="IO"></a>IO</h3><p>IO通常包括磁盘IO和网络IO。深度学习的训练中，通常不会涉及大量的网络吞吐。多数情况下，可能导致的网络IO瓶颈是分布式训练的过程中。</p>
<h4 id="磁盘IO"><a href="#磁盘IO" class="headerlink" title="磁盘IO"></a>磁盘IO</h4><p>磁盘IO瓶颈常见于在大数据集和高性能显卡的环境下。这样的情况下，GPU运算速度非常快，而数据集很大，导致数据加载的速度跟不上数据运算的速度。</p>
<p>如何判断你的程序遇到了磁盘IO的瓶颈？一种方法是检查系统的<code>iowait</code>。<code>iowait</code>表示CPU等待磁盘的时间，有关具体的含义，请参考<a href="https://serverfault.com/questions/12679/can-anyone-explain-precisely-what-iowait-is" target="_blank" rel="noopener">这里的讨论</a>. </p>
<p>可以通过命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">iostat -x 1</span><br></pre></td></tr></table></figure>

<p>观察每个磁盘的IO情况以及总的CPU iowait:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           0.12    0.00    0.21    0.04    0.00   99.62</span><br><span class="line"></span><br><span class="line">Device:         rrqm&#x2F;s   wrqm&#x2F;s     r&#x2F;s     w&#x2F;s    rkB&#x2F;s    wkB&#x2F;s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">loop0             0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br><span class="line">sdg               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br><span class="line">sdb               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br><span class="line">sdd               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br><span class="line">dm-0              0.00     0.00    0.00    1.00     0.00     4.00     8.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br><span class="line">dm-1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br></pre></td></tr></table></figure>

<blockquote>
<p>iostat没有在Ubuntu系统上预先安装。对于Ubuntu系统，请预先安装此程序：<code>sudo apt-get install sysstat</code></p>
</blockquote>
<p>一般来说，如果<code>iowait</code>长期保持100%，就是磁盘瓶颈的警告。也可以通过观察<code>avgqu-sz</code>字段判断IO队列长度。[<code>iostat</code>]命令的其他字段的含义请参考<a href="https://linux.die.net/man/1/iostat" target="_blank" rel="noopener">文档</a>或<a href="https://blog.serverfault.com/2010/07/06/777852755/" target="_blank" rel="noopener">这篇博客</a>.</p>
<h4 id="分布式训练"><a href="#分布式训练" class="headerlink" title="分布式训练"></a>分布式训练</h4><p>分布式训练有可能会出现网络IO的瓶颈。Linux有大量监控网络流量的方法，例如<a href="https://github.com/raboof/nethogs" target="_blank" rel="noopener"><code>nethogs</code></a>，以及这里的讨论<a href="https://askubuntu.com/questions/257263/how-to-display-network-traffic-in-the-terminal。" target="_blank" rel="noopener">https://askubuntu.com/questions/257263/how-to-display-network-traffic-in-the-terminal。</a></p>
<p>一般来说，在分布式训练时，如果网络流量长期处于顶点（例如千兆网络一直被占满），那么就有理由猜测可能是网络限制了分布式训练的过程，包括数据和模型参数的传输过程。此情况可能在大数据集和大模型训练的时候发生。</p>
<h3 id="特殊数据类型的加载方式"><a href="#特殊数据类型的加载方式" class="headerlink" title="特殊数据类型的加载方式"></a>特殊数据类型的加载方式</h3><h4 id="Tensor-or-numpy-array"><a href="#Tensor-or-numpy-array" class="headerlink" title="Tensor (or numpy array)"></a>Tensor (or numpy array)</h4><p>在一些情况下，整个数据集都被以<a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html" target="_blank" rel="noopener"><code>numpy.ndarray</code></a>的形式保存在内存中，在不同的step中从numpy数组中<a href="https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html" target="_blank" rel="noopener">索引</a>出不同的数据子集训练。PyTorch提供了<code>TensorDataset</code>来做到这一点。理论上，直接从内存中读取数据不应当有时间瓶颈。然而，PyTorch的<code>TensorDataset</code>配合<code>DataLoader</code>使用可能会发生数据加载过慢的情况。分析及解决方案请参考<a href="https://huangbiubiu.github.io/2019/BEST-PRACTICE-PyTorch-TensorDataset/" target="_blank" rel="noopener">这篇博文</a>.</p>
<h4 id="CSV-文件"><a href="#CSV-文件" class="headerlink" title="CSV 文件"></a>CSV 文件</h4><p>大量结构化数据集以csv的格式提供。然而，对于SGD这类批优化算法，csv的速度是缓慢的。原因在于，不同于固定长度的数据类型（相同大小的图片，数组等），csv通过换行符<code>\n</code>来确定两行之间的分界点。<a href="https://en.wikipedia.org/wiki/Random_access" target="_blank" rel="noopener">随机读取</a>一个csv文件的某一行的时间开销是$O(n)$ (数组是$O(1)$). </p>
<h2 id="怎么做：如何提高数据加载的速度"><a href="#怎么做：如何提高数据加载的速度" class="headerlink" title="怎么做：如何提高数据加载的速度"></a>怎么做：如何提高数据加载的速度</h2><p>面对不同的原因，列出不同的解决方法如下。</p>
<h3 id="数据增强和预处理-1"><a href="#数据增强和预处理-1" class="headerlink" title="数据增强和预处理"></a>数据增强和预处理</h3><ol>
<li>考虑提高运行效率，包括但不限于使用多进程(线程)处理等</li>
<li>考虑将数据增强和预处理操作离线进行，即预先处理好数据存在磁盘中，在训练时直接加载处理好的数据。</li>
</ol>
<h3 id="IO-1"><a href="#IO-1" class="headerlink" title="IO"></a>IO</h3><h4 id="磁盘IO-1"><a href="#磁盘IO-1" class="headerlink" title="磁盘IO"></a>磁盘IO</h4><ol>
<li><p>将数据存储在SSD中通常能解决此问题。</p>
</li>
<li><p>如果内存足够大，考虑将所有数据加载到内存中。</p>
<p> 通常情况下，在内存足够的时候，操作系统会自动将常用的文件缓存到内存中。这样的缓存机制在<em>大多数情况</em>下工作得很好。然而，在对大量数据进行随机读写时，文件系统的缓存可能无法满足我们的要求。此时我们可以人工将数据强制固定在内存中。使用ramfs可以做到这一点，请参考<a href="https://unix.stackexchange.com/questions/66329/creating-a-ram-disk-on-linux。" target="_blank" rel="noopener">https://unix.stackexchange.com/questions/66329/creating-a-ram-disk-on-linux。</a></p>
</li>
<li><p>如果你有多块磁盘，考虑使用RAID。效率最高的RAID是使用<a href="https://en.wikipedia.org/wiki/Disk_array_controller" target="_blank" rel="noopener">专门的RAID控制器</a>。最方便的方法是使用<a href="https://en.wikipedia.org/wiki/RAID#Software-based" target="_blank" rel="noopener">软RAID</a>，即通过软件的方式模拟磁盘阵列。主流的操作系统都提供了软RAID的支持，如<a href="https://superuser.com/questions/1001042/software-raid-windows-10" target="_blank" rel="noopener">Windows</a>, <a href="https://www.digitalocean.com/community/tutorials/how-to-create-raid-arrays-with-mdadm-on-ubuntu-16-04" target="_blank" rel="noopener">Ubuntu</a>等。</p>
</li>
</ol>
<h4 id="分布式训练-1"><a href="#分布式训练-1" class="headerlink" title="分布式训练"></a>分布式训练</h4><ol>
<li>升级为高速网络连接，如万兆以太网、<a href="https://zh.wikipedia.org/wiki/InfiniBand" target="_blank" rel="noopener">InfiniBand</a>.</li>
<li>使用单机训练.</li>
</ol>
<h3 id="特殊数据类型"><a href="#特殊数据类型" class="headerlink" title="特殊数据类型"></a>特殊数据类型</h3><h4 id="CSV"><a href="#CSV" class="headerlink" title="CSV"></a>CSV</h4><p>推荐使用<a href="www.hdfgroup.org">HDF5格式</a>存储数据。如果csv文件已经使用pandas <code>DataFrame</code>读取，仅需使用<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_hdf.html" target="_blank" rel="noopener"><code>to_hdf</code></a>方法即可将<code>DataFrame</code>保存为HDF5格式的文件.</p>
<blockquote>
<p>Note: 更快的Python结构化数据处理<br><br><br>对于使用Python处理大量的数据，请参考:<br><br>[1] <a href="https://stackoverflow.com/questions/14262433/large-data-work-flows-using-pandas" target="_blank" rel="noopener">https://stackoverflow.com/questions/14262433/large-data-work-flows-using-pandas</a><br><br>[2] Python上的高性能计算框架<a href="dask.org">Dask</a><br><br>[3] 高性能类Pandas库<a href="https://github.com/modin-project/modin" target="_blank" rel="noopener">Modin</a><br><br>[4] Python加速框架<a href="numba.pydata.org">Numba</a><br></p>
</blockquote>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="Checklist"><a href="#Checklist" class="headerlink" title="Checklist"></a>Checklist</h3><p>如何检查并消除数据加载瓶颈：</p>
<ul>
<li><a href="#间接观察：nvidia-smi">使用 <code>nvidia-smi</code> 快速定性判断是否显卡是否被充分利用</a></li>
<li><a href="#直接测量-time-time">添加时间测试代码定量判断数据加载时间</a></li>
<li><a href="#使用Profile工具-cProfile">使用<code>cProfile</code>定位具体瓶颈代码</a></li>
<li>依据<a href="#为什么：什么操作会拖慢数据加载？">不同情况</a>采用<a href="#怎么做：如何提高数据加载的速度">不同的解决方案</a></li>
<li>Enjoy fast training!</li>
</ul>
]]></content>
      <categories>
        <category>TUTORIAL</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
        <tag>As Fast As Possible</tag>
        <tag>TUTORIAL</tag>
      </tags>
  </entry>
  <entry>
    <title>Faster PyTorch TensorDataset</title>
    <url>/2019/BEST-PRACTICE-PyTorch-TensorDataset/</url>
    <content><![CDATA[<!-- # [BEST PRACTICE] PyTorch TensorDataset -->

<p>一个更加快速的<code>TensorDataset</code>使用方法, 70x速度提升!</p>
<a id="more"></a>

<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>PyTorch的<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset" target="_blank" rel="noopener"><code>Dataset</code></a>类提供了非常好用的数据加载接口。<code>TensorDataset</code>继承了<code>Dataset</code>，提供了已经完全加载到内存中的矩阵的数据读取接口。一个普遍的使用方法是这样的:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset, DataLoader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data_all = np.random.rand(<span class="number">100000</span>, <span class="number">128</span>)  <span class="comment"># demo input</span></span><br><span class="line">dataset = TensorDataset(torch.from_numpy(data_all))</span><br><span class="line">data_loader = DataLoader(dataset=dataset, </span><br><span class="line">                            shuffle=<span class="literal">True</span>,</span><br><span class="line">                            batch_size=<span class="number">8192</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> data_loader:</span><br><span class="line">    <span class="comment"># training</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<p>测量一下数据读取的速度(代码见附录1), 5次运行结果的平均值（单位是秒）:</p>
<blockquote>
<p>37.70331398304552</p>
</blockquote>
<h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h2><p>5次运行的平均速度是37.7s， <code>batch size</code>是8192，一个<code>epoch</code>100个step，平均每个step花费的时间是0.3s. 这个时间对于很多训练任务是无法接受的。作为参考，我们使用4*Tesla V100在ImageNet上训练ResNet 50，每个step的时间是0.27s. 在这个情况下，上述方法读取数据的时间已经超过了模型forward和backward的时间，极大拖慢了运行速度. </p>
<p>问题出在哪里？<code>TensorDataset</code>中，数据全部存储在内存中，每次需要数据的时候直接从内存中取出相应的数据即可，不存在IO瓶颈的问题。</p>
<p>问题在于，对于<code>DataLoader</code>，每次调用<code>Dataset</code>中一个值的时候，循环地调用<code>Dataset</code>的<code>__getitem__</code>函数，类似于以下这种写法:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># only a simplified demo</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_next_batch</span><span class="params">()</span>:</span></span><br><span class="line">    results = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(indices):</span><br><span class="line">        results.append(dataset[i])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> torch.cat(results) <span class="comment"># default collate_fn</span></span><br></pre></td></tr></table></figure>

<p>这样写对于需要从磁盘中读取的数据是没有问题的，但是对于<code>Tensor</code>，我们知道有更高效的写法:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_next_batch</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> data[indices]</span><br></pre></td></tr></table></figure>

<p>这个问题在PyTorch issue中已有<a href="https://github.com/pytorch/pytorch/issues/4959" target="_blank" rel="noopener">相关讨论</a>.</p>
<h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><p>那么关键就在于如何在尽可能少地改动代码的情况下保证<code>DataLoader</code>使用自定义的index. 我们使用<a href="https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler" target="_blank" rel="noopener"><code>Sampler</code></a>控制<code>DataLoader</code>的采样方法，一次返回一批<code>Tensor</code>，而不是一次返回一条数据然后再concat起来. 此方法参考了<a href="https://github.com/pytorch/pytorch/issues/4959#issuecomment-362424598" target="_blank" rel="noopener">@fmassa的回复</a>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TensorSampler</span><span class="params">(Sampler)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data_source: Sized, batch_size=<span class="number">8192</span>)</span>:</span></span><br><span class="line">        super().__init__(data_source)</span><br><span class="line">        self.data_source = data_source</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> iter(torch.randperm(len(self.data_source)).long().split(self.batch_size))</span><br></pre></td></tr></table></figure>

<p>这样一次<code>__getitem__</code>就会返回一个batch的数据。此时需要<a href="https://pytorch.org/docs/stable/data.html#disable-automatic-batching" target="_blank" rel="noopener">禁用<code>DataLoader</code>的自动batch</a>, 由Sampler来控制batch:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_loader = DataLoader(dataset=dataset, shuffle=<span class="literal">False</span>,</span><br><span class="line">                        batch_size=<span class="literal">None</span>, batch_sampler=<span class="literal">None</span>,</span><br><span class="line">                        sampler=TensorSampler(data_source=dataset,</span><br><span class="line">                                              batch_size=batch_size))</span><br></pre></td></tr></table></figure>

<p>修改后的代码参见附录2. </p>
<blockquote>
<p>Note: 在使用自定义Sampler时，<code>DataLoader</code>的shuffle选项将不可用。</p>
</blockquote>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>重新运行优化过的代码，计算运行时间为（评估代码见附录2）:</p>
<blockquote>
<p>0.5254814000800252</p>
</blockquote>
<p>速度提升了<strong>70</strong>倍.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>在使用<code>TensorDataset</code>时，应尽量避免直接使用<code>Dataloader</code>，否则<code>Dataloader</code>的auto-batch机制会导致数据加载非常缓慢。一种可行的方法是使用自定义的<code>Sampler</code>控制每次从<code>Dataset</code>中的采样方式，一次直接取出一个batch的数据.</p>
<h2 id="Further-Reading"><a href="#Further-Reading" class="headerlink" title="Further Reading"></a>Further Reading</h2><ol>
<li>PyTorch社区的相关讨论 <a href="https://github.com/pytorch/pytorch/issues/4959" target="_blank" rel="noopener">https://github.com/pytorch/pytorch/issues/4959</a></li>
<li><code>torch.utils.data</code>文档 <a href="https://pytorch.org/docs/stable/data.html#disable-automatic-batching" target="_blank" rel="noopener">https://pytorch.org/docs/stable/data.html#disable-automatic-batching</a></li>
</ol>
<h2 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h2><h3 id="代码1-评估代码运行速度"><a href="#代码1-评估代码运行速度" class="headerlink" title="代码1: 评估代码运行速度"></a>代码1: 评估代码运行速度</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> timeit</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset, DataLoader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_data</span><span class="params">()</span> -&gt; DataLoader:</span></span><br><span class="line">    batch_size = <span class="number">8192</span></span><br><span class="line"></span><br><span class="line">    data_all = np.random.rand(batch_size * <span class="number">100</span>, <span class="number">128</span>)  <span class="comment"># demo input</span></span><br><span class="line">    dataset = TensorDataset(torch.from_numpy(data_all))</span><br><span class="line">    data_loader = DataLoader(dataset=dataset,</span><br><span class="line">                             shuffle=<span class="literal">True</span>,</span><br><span class="line">                             batch_size=<span class="number">8192</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> data_loader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iterate_data</span><span class="params">(dataloader)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i, x <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">        <span class="comment"># training</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dataloader = prepare_data()</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    print(timeit.timeit(<span class="string">'iterate_data(dataloader)'</span>, globals=globals(), number=<span class="number">5</span>))</span><br></pre></td></tr></table></figure>

<h3 id="代码2-评估改进后代码运行速度"><a href="#代码2-评估改进后代码运行速度" class="headerlink" title="代码2: 评估改进后代码运行速度"></a>代码2: 评估改进后代码运行速度</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> timeit</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Sized</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset, DataLoader, Sampler</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TensorSampler</span><span class="params">(Sampler)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data_source: Sized, batch_size=<span class="number">8192</span>)</span>:</span></span><br><span class="line">        super().__init__(data_source)</span><br><span class="line">        self.data_source = data_source</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> iter(torch.randperm(len(self.data_source)).long().split(self.batch_size))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_data</span><span class="params">()</span> -&gt; DataLoader:</span></span><br><span class="line">    batch_size = <span class="number">8192</span></span><br><span class="line"></span><br><span class="line">    data_all = np.random.rand(batch_size * <span class="number">100</span>, <span class="number">128</span>)  <span class="comment"># demo input</span></span><br><span class="line">    dataset = TensorDataset(torch.from_numpy(data_all))</span><br><span class="line"></span><br><span class="line">    data_loader = DataLoader(dataset=dataset, shuffle=<span class="literal">False</span>,</span><br><span class="line">                             batch_size=<span class="literal">None</span>, batch_sampler=<span class="literal">None</span>,</span><br><span class="line">                             sampler=TensorSampler(data_source=dataset,</span><br><span class="line">                                                   batch_size=batch_size))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> data_loader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iterate_data</span><span class="params">(dataloader)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i, x <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">        <span class="comment"># training</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dataloader = prepare_data()</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    print(timeit.timeit(<span class="string">'iterate_data(dataloader)'</span>, globals=globals(), number=<span class="number">5</span>))</span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>BEST PRACTICE</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
        <tag>BEST PRACTICE</tag>
        <tag>As Fast As Possible</tag>
      </tags>
  </entry>
</search>
